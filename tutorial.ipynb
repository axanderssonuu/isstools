{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial Steps:\n",
    "\n",
    "1. **Download Example ISS Dataset:** Obtain the provided ISS dataset to work with.\n",
    "\n",
    "2. **Optional: Deconvolution and Maximum Intensity Projection:** You have the option to apply deconvolution and create maximum intensity projections from the raw image data.\n",
    "\n",
    "3. **Stitching Image Data:** Combine the image data using stitching techniques.\n",
    "\n",
    "4. **Decode Image Data:** Decode the stitched image data.\n",
    "\n",
    "5. **Quality Control and Visualization:** Evaluate the results through quality control measures and visualize them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation\n",
    "\n",
    "We recommened installing all the necessary packages using [miniconda](https://docs.conda.io/en/latest/miniconda.html)\n",
    "or [Anaconda](https://www.anaconda.com/products/individual)\n",
    "\n",
    "Begin by creating a named conda environment with python 3.10:\n",
    "```bash\n",
    "conda create -y -n iss_decoding_tutorial python=3.10\n",
    "```\n",
    "\n",
    "Activate the conda environment:\n",
    "```bash\n",
    "conda activate iss_decoding_tutorial\n",
    "```\n",
    "\n",
    "In the activated environment, install the following packages:\n",
    "```bash\n",
    "conda install -y -c conda-forge numpy scipy matplotlib networkx scikit-image=0.19 scikit-learn \"tifffile>=2023.3.15\" zarr pyjnius blessed\n",
    "pip install ashlar pandas tqdm seaborn torch napari[all]\n",
    "```\n",
    "\n",
    "We also need to install `libvips` and `pyvips`. On Linux and macOS, this can be done through conda:\n",
    "\n",
    "```bash\n",
    "conda install -c conda-forge libvips pyvips\n",
    "```\n",
    "\n",
    "on Windows you can download a pre-compiled binary from the libvips website.\n",
    "\n",
    "https://libvips.github.io/libvips/install.html\n",
    "\n",
    "\n",
    "OBS! You will also need to add `vips-dev-x.y\\bin` to your PATH so that pyvips can find all the DLLs it needs. You can either do this in the Advanced System Settings control panel,\n",
    "\n",
    "Next, install `pyvips` as\n",
    "\n",
    "```bash\n",
    "pip install pyvips\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Download ISS Data\n",
    "To begin, download the ISS toy dataset by clicking on the following link: [ISS Toy Dataset](https://drive.google.com/file/d/1zYoUHDOCIuvyJBWj-KQnbVMM4THBf7ll/view?usp=drive_link).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the dataset is downloaded, take a moment to examine the file names and familiarize yourself with their naming conventions. The files adhere to the following naming pattern: `stage{stage}_round{round}_z{z}_channel{channel}.tif`, where the placeholders correspond to the numerical identifiers for the stage position, staining round, z level, and channel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll proceed to load the dataset into an `ISSDataContainer` class. This class is designed to facilitate dataset management without the need to load the entire contents into memory simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added datasets\\tutorial\\decoding_tutorial\\S0_R0_C0_Z17.tif. Stage: 0, Round: 0, Channel: 0\n",
      "Added datasets\\tutorial\\decoding_tutorial\\S0_R0_C1_Z17.tif. Stage: 0, Round: 0, Channel: 1\n",
      "Added datasets\\tutorial\\decoding_tutorial\\S0_R0_C2_Z17.tif. Stage: 0, Round: 0, Channel: 2\n",
      "Added datasets\\tutorial\\decoding_tutorial\\S0_R0_C3_Z17.tif. Stage: 0, Round: 0, Channel: 3\n",
      "Added datasets\\tutorial\\decoding_tutorial\\S0_R0_C4_Z17.tif. Stage: 0, Round: 0, Channel: 4\n",
      "Added datasets\\tutorial\\decoding_tutorial\\S1_R0_C0_Z17.tif. Stage: 1, Round: 0, Channel: 0\n",
      "Added datasets\\tutorial\\decoding_tutorial\\S1_R0_C1_Z17.tif. Stage: 1, Round: 0, Channel: 1\n",
      "Added datasets\\tutorial\\decoding_tutorial\\S1_R0_C2_Z17.tif. Stage: 1, Round: 0, Channel: 2\n",
      "Added datasets\\tutorial\\decoding_tutorial\\S1_R0_C3_Z17.tif. Stage: 1, Round: 0, Channel: 3\n",
      "Added datasets\\tutorial\\decoding_tutorial\\S1_R0_C4_Z17.tif. Stage: 1, Round: 0, Channel: 4\n",
      "Added datasets\\tutorial\\decoding_tutorial\\S0_R1_C0_Z17.tif. Stage: 0, Round: 1, Channel: 0\n",
      "Added datasets\\tutorial\\decoding_tutorial\\S0_R1_C1_Z17.tif. Stage: 0, Round: 1, Channel: 1\n",
      "Added datasets\\tutorial\\decoding_tutorial\\S0_R1_C2_Z17.tif. Stage: 0, Round: 1, Channel: 2\n",
      "Added datasets\\tutorial\\decoding_tutorial\\S0_R1_C3_Z17.tif. Stage: 0, Round: 1, Channel: 3\n",
      "Added datasets\\tutorial\\decoding_tutorial\\S0_R1_C4_Z17.tif. Stage: 0, Round: 1, Channel: 4\n",
      "Added datasets\\tutorial\\decoding_tutorial\\S1_R1_C0_Z17.tif. Stage: 1, Round: 1, Channel: 0\n",
      "Added datasets\\tutorial\\decoding_tutorial\\S1_R1_C1_Z17.tif. Stage: 1, Round: 1, Channel: 1\n",
      "Added datasets\\tutorial\\decoding_tutorial\\S1_R1_C2_Z17.tif. Stage: 1, Round: 1, Channel: 2\n",
      "Added datasets\\tutorial\\decoding_tutorial\\S1_R1_C3_Z17.tif. Stage: 1, Round: 1, Channel: 3\n",
      "Added datasets\\tutorial\\decoding_tutorial\\S1_R1_C4_Z17.tif. Stage: 1, Round: 1, Channel: 4\n",
      "Added datasets\\tutorial\\decoding_tutorial\\S0_R2_C0_Z17.tif. Stage: 0, Round: 2, Channel: 0\n",
      "Added datasets\\tutorial\\decoding_tutorial\\S0_R2_C1_Z17.tif. Stage: 0, Round: 2, Channel: 1\n",
      "Added datasets\\tutorial\\decoding_tutorial\\S0_R2_C2_Z17.tif. Stage: 0, Round: 2, Channel: 2\n",
      "Added datasets\\tutorial\\decoding_tutorial\\S0_R2_C3_Z17.tif. Stage: 0, Round: 2, Channel: 3\n",
      "Added datasets\\tutorial\\decoding_tutorial\\S0_R2_C4_Z17.tif. Stage: 0, Round: 2, Channel: 4\n",
      "Added datasets\\tutorial\\decoding_tutorial\\S1_R2_C0_Z17.tif. Stage: 1, Round: 2, Channel: 0\n",
      "Added datasets\\tutorial\\decoding_tutorial\\S1_R2_C1_Z17.tif. Stage: 1, Round: 2, Channel: 1\n",
      "Added datasets\\tutorial\\decoding_tutorial\\S1_R2_C2_Z17.tif. Stage: 1, Round: 2, Channel: 2\n",
      "Added datasets\\tutorial\\decoding_tutorial\\S1_R2_C3_Z17.tif. Stage: 1, Round: 2, Channel: 3\n",
      "Added datasets\\tutorial\\decoding_tutorial\\S1_R2_C4_Z17.tif. Stage: 1, Round: 2, Channel: 4\n",
      "Added datasets\\tutorial\\decoding_tutorial\\S0_R3_C0_Z17.tif. Stage: 0, Round: 3, Channel: 0\n",
      "Added datasets\\tutorial\\decoding_tutorial\\S0_R3_C1_Z17.tif. Stage: 0, Round: 3, Channel: 1\n",
      "Added datasets\\tutorial\\decoding_tutorial\\S0_R3_C2_Z17.tif. Stage: 0, Round: 3, Channel: 2\n",
      "Added datasets\\tutorial\\decoding_tutorial\\S0_R3_C3_Z17.tif. Stage: 0, Round: 3, Channel: 3\n",
      "Added datasets\\tutorial\\decoding_tutorial\\S0_R3_C4_Z17.tif. Stage: 0, Round: 3, Channel: 4\n",
      "Added datasets\\tutorial\\decoding_tutorial\\S1_R3_C0_Z17.tif. Stage: 1, Round: 3, Channel: 0\n",
      "Added datasets\\tutorial\\decoding_tutorial\\S1_R3_C1_Z17.tif. Stage: 1, Round: 3, Channel: 1\n",
      "Added datasets\\tutorial\\decoding_tutorial\\S1_R3_C2_Z17.tif. Stage: 1, Round: 3, Channel: 2\n",
      "Added datasets\\tutorial\\decoding_tutorial\\S1_R3_C3_Z17.tif. Stage: 1, Round: 3, Channel: 3\n",
      "Added datasets\\tutorial\\decoding_tutorial\\S1_R3_C4_Z17.tif. Stage: 1, Round: 3, Channel: 4\n",
      "Added datasets\\tutorial\\decoding_tutorial\\S0_R4_C0_Z17.tif. Stage: 0, Round: 4, Channel: 0\n",
      "Added datasets\\tutorial\\decoding_tutorial\\S0_R4_C1_Z17.tif. Stage: 0, Round: 4, Channel: 1\n",
      "Added datasets\\tutorial\\decoding_tutorial\\S0_R4_C2_Z17.tif. Stage: 0, Round: 4, Channel: 2\n",
      "Added datasets\\tutorial\\decoding_tutorial\\S0_R4_C3_Z17.tif. Stage: 0, Round: 4, Channel: 3\n",
      "Added datasets\\tutorial\\decoding_tutorial\\S0_R4_C4_Z17.tif. Stage: 0, Round: 4, Channel: 4\n",
      "Added datasets\\tutorial\\decoding_tutorial\\S1_R4_C0_Z17.tif. Stage: 1, Round: 4, Channel: 0\n",
      "Added datasets\\tutorial\\decoding_tutorial\\S1_R4_C1_Z17.tif. Stage: 1, Round: 4, Channel: 1\n",
      "Added datasets\\tutorial\\decoding_tutorial\\S1_R4_C2_Z17.tif. Stage: 1, Round: 4, Channel: 2\n",
      "Added datasets\\tutorial\\decoding_tutorial\\S1_R4_C3_Z17.tif. Stage: 1, Round: 4, Channel: 3\n",
      "Added datasets\\tutorial\\decoding_tutorial\\S1_R4_C4_Z17.tif. Stage: 1, Round: 4, Channel: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tools.image_container.ISSDataContainer at 0x11a2fe1a440>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tools.image_container import ISSDataContainer\n",
    "from os.path import join\n",
    "# Create the container\n",
    "issdata = ISSDataContainer()\n",
    "\n",
    "# Add images\n",
    "# join('downloads', 'stage{stage}_rounds{round}_z{z}_channel{channel}.tif')\n",
    "pattern = join('datasets', 'tutorial', 'decoding_tutorial', 'S{stage}_R{round}_C{channel}_Z{z}.tif') \n",
    "issdata.add_images_from_filepattern(pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For verification, you can print out the size of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 number of stages\n",
      "There are 5 number of rounds\n",
      "There are 5 number of channels\n"
     ]
    }
   ],
   "source": [
    "num_stages, num_rounds, num_channels = issdata.get_dataset_shape()\n",
    "print(f'There are {num_stages} number of stages')\n",
    "print(f'There are {num_rounds} number of rounds')\n",
    "print(f'There are {num_channels} number of channels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also verify that there are equal number of images for each stage, round and channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issdata.is_dataset_complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional) Let's take a look at the data using Napari."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    import napari\n",
    "\n",
    "    # Select small piece of the data\n",
    "    small_data = issdata.select(stage=0, round=0)\n",
    "\n",
    "    # Load images into memory\n",
    "    small_data.load()\n",
    "\n",
    "    # Run Napari\n",
    "    viewer = napari.Viewer()\n",
    "    viewer.add_image(small_data.data.squeeze())\n",
    "    napari.run()\n",
    "\n",
    "    # Free memory\n",
    "    small_data.unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. 2D Projection\n",
    "\n",
    "In this step, we will perform a 2D projection of our data through maximum intensity projection. This involves selecting the maximum pixel value across different z-planes. To enhance the clarity of the 2D images, we can apply deconvolution. It's worth noting that deconvolution can be applied either before or after the 2D projection. However, it's important to highlight that deconvolution can be computationally intensive, often requiring a CUDA-supported GPU for efficient processing, especially when dealing with substantial stacks of 3D multiplexed images. For the purpose of this tutorial, we will omit the deconvolution step, but the necessary functions can be found in the `deconvolution.py` file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The iterate dataset allows us to iterate the dataset over stages, rounds and channels.\n",
    "import numpy as np\n",
    "from tools.utils import imwrite\n",
    "from os.path import join\n",
    "\n",
    "for index, small_dataset in issdata.iterate_dataset(iter_stages=True, iter_rounds=True, iter_channels=True):\n",
    "    # Load the small dataset\n",
    "    small_dataset.load()\n",
    "    # Get the image data\n",
    "    data = small_dataset.data\n",
    "    # MIP the data\n",
    "    data = np.squeeze(data).max(axis=0)\n",
    "    # Save the data\n",
    "    imwrite(join('datasets','tutorial', 'mipped', 'S{stage}_R{round}_C{channel}.tif'.format(**index)), data)\n",
    "    # Finally, we unload the images (otherwise we might run oom)\n",
    "    small_dataset.unload()\n",
    "\n",
    "# Or equivalently ...\n",
    "# from ISSDataset import mip\n",
    "# mip(join('mip','stage{stage}_round{round}_channel{channel}.tif'), issdata)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Stitching\n",
    "\n",
    "We will proceed to stitch the data using ASHLAR. This task can be accomplished by utilizing the `stitch_ashlar.py` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added datasets\\tutorial\\mipped\\S0_R0_C0.tif. Stage: 0, Round: 0, Channel: 0\n",
      "Added datasets\\tutorial\\mipped\\S0_R0_C1.tif. Stage: 0, Round: 0, Channel: 1\n",
      "Added datasets\\tutorial\\mipped\\S0_R0_C2.tif. Stage: 0, Round: 0, Channel: 2\n",
      "Added datasets\\tutorial\\mipped\\S0_R0_C3.tif. Stage: 0, Round: 0, Channel: 3\n",
      "Added datasets\\tutorial\\mipped\\S0_R0_C4.tif. Stage: 0, Round: 0, Channel: 4\n",
      "Added datasets\\tutorial\\mipped\\S1_R0_C0.tif. Stage: 1, Round: 0, Channel: 0\n",
      "Added datasets\\tutorial\\mipped\\S1_R0_C1.tif. Stage: 1, Round: 0, Channel: 1\n",
      "Added datasets\\tutorial\\mipped\\S1_R0_C2.tif. Stage: 1, Round: 0, Channel: 2\n",
      "Added datasets\\tutorial\\mipped\\S1_R0_C3.tif. Stage: 1, Round: 0, Channel: 3\n",
      "Added datasets\\tutorial\\mipped\\S1_R0_C4.tif. Stage: 1, Round: 0, Channel: 4\n",
      "Added datasets\\tutorial\\mipped\\S0_R1_C0.tif. Stage: 0, Round: 1, Channel: 0\n",
      "Added datasets\\tutorial\\mipped\\S0_R1_C1.tif. Stage: 0, Round: 1, Channel: 1\n",
      "Added datasets\\tutorial\\mipped\\S0_R1_C2.tif. Stage: 0, Round: 1, Channel: 2\n",
      "Added datasets\\tutorial\\mipped\\S0_R1_C3.tif. Stage: 0, Round: 1, Channel: 3\n",
      "Added datasets\\tutorial\\mipped\\S0_R1_C4.tif. Stage: 0, Round: 1, Channel: 4\n",
      "Added datasets\\tutorial\\mipped\\S1_R1_C0.tif. Stage: 1, Round: 1, Channel: 0\n",
      "Added datasets\\tutorial\\mipped\\S1_R1_C1.tif. Stage: 1, Round: 1, Channel: 1\n",
      "Added datasets\\tutorial\\mipped\\S1_R1_C2.tif. Stage: 1, Round: 1, Channel: 2\n",
      "Added datasets\\tutorial\\mipped\\S1_R1_C3.tif. Stage: 1, Round: 1, Channel: 3\n",
      "Added datasets\\tutorial\\mipped\\S1_R1_C4.tif. Stage: 1, Round: 1, Channel: 4\n",
      "Added datasets\\tutorial\\mipped\\S0_R2_C0.tif. Stage: 0, Round: 2, Channel: 0\n",
      "Added datasets\\tutorial\\mipped\\S0_R2_C1.tif. Stage: 0, Round: 2, Channel: 1\n",
      "Added datasets\\tutorial\\mipped\\S0_R2_C2.tif. Stage: 0, Round: 2, Channel: 2\n",
      "Added datasets\\tutorial\\mipped\\S0_R2_C3.tif. Stage: 0, Round: 2, Channel: 3\n",
      "Added datasets\\tutorial\\mipped\\S0_R2_C4.tif. Stage: 0, Round: 2, Channel: 4\n",
      "Added datasets\\tutorial\\mipped\\S1_R2_C0.tif. Stage: 1, Round: 2, Channel: 0\n",
      "Added datasets\\tutorial\\mipped\\S1_R2_C1.tif. Stage: 1, Round: 2, Channel: 1\n",
      "Added datasets\\tutorial\\mipped\\S1_R2_C2.tif. Stage: 1, Round: 2, Channel: 2\n",
      "Added datasets\\tutorial\\mipped\\S1_R2_C3.tif. Stage: 1, Round: 2, Channel: 3\n",
      "Added datasets\\tutorial\\mipped\\S1_R2_C4.tif. Stage: 1, Round: 2, Channel: 4\n",
      "Added datasets\\tutorial\\mipped\\S0_R3_C0.tif. Stage: 0, Round: 3, Channel: 0\n",
      "Added datasets\\tutorial\\mipped\\S0_R3_C1.tif. Stage: 0, Round: 3, Channel: 1\n",
      "Added datasets\\tutorial\\mipped\\S0_R3_C2.tif. Stage: 0, Round: 3, Channel: 2\n",
      "Added datasets\\tutorial\\mipped\\S0_R3_C3.tif. Stage: 0, Round: 3, Channel: 3\n",
      "Added datasets\\tutorial\\mipped\\S0_R3_C4.tif. Stage: 0, Round: 3, Channel: 4\n",
      "Added datasets\\tutorial\\mipped\\S1_R3_C0.tif. Stage: 1, Round: 3, Channel: 0\n",
      "Added datasets\\tutorial\\mipped\\S1_R3_C1.tif. Stage: 1, Round: 3, Channel: 1\n",
      "Added datasets\\tutorial\\mipped\\S1_R3_C2.tif. Stage: 1, Round: 3, Channel: 2\n",
      "Added datasets\\tutorial\\mipped\\S1_R3_C3.tif. Stage: 1, Round: 3, Channel: 3\n",
      "Added datasets\\tutorial\\mipped\\S1_R3_C4.tif. Stage: 1, Round: 3, Channel: 4\n",
      "Added datasets\\tutorial\\mipped\\S0_R4_C0.tif. Stage: 0, Round: 4, Channel: 0\n",
      "Added datasets\\tutorial\\mipped\\S0_R4_C1.tif. Stage: 0, Round: 4, Channel: 1\n",
      "Added datasets\\tutorial\\mipped\\S0_R4_C2.tif. Stage: 0, Round: 4, Channel: 2\n",
      "Added datasets\\tutorial\\mipped\\S0_R4_C3.tif. Stage: 0, Round: 4, Channel: 3\n",
      "Added datasets\\tutorial\\mipped\\S0_R4_C4.tif. Stage: 0, Round: 4, Channel: 4\n",
      "Added datasets\\tutorial\\mipped\\S1_R4_C0.tif. Stage: 1, Round: 4, Channel: 0\n",
      "Added datasets\\tutorial\\mipped\\S1_R4_C1.tif. Stage: 1, Round: 4, Channel: 1\n",
      "Added datasets\\tutorial\\mipped\\S1_R4_C2.tif. Stage: 1, Round: 4, Channel: 2\n",
      "Added datasets\\tutorial\\mipped\\S1_R4_C3.tif. Stage: 1, Round: 4, Channel: 3\n",
      "Added datasets\\tutorial\\mipped\\S1_R4_C4.tif. Stage: 1, Round: 4, Channel: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tools.image_container.ISSDataContainer at 0x11a4f7f2e00>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from os.path import join\n",
    "# First we load the miped data\n",
    "iss_data_miped = ISSDataContainer()\n",
    "iss_data_miped.add_images_from_filepattern(join('datasets','tutorial', 'mipped','S{stage}_R{round}_C{channel}.tif'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To successfully register and stitch the image data, it's crucial to have access to the initial position of each stage in pixel coordinates. This information can typically be extracted from the microscope software.\n",
    "\n",
    "FAQ 1: If you get an error saying `Exception: Unable to find JAVA_HOME` you have to install OpenJDK11. On Windows, OpenJDK can be downloaded from [here](https://learn.microsoft.com/en-us/java/openjdk/download#openjdk-11). on Linux and macOS, see [this](https://openjdk.org/install/). Perhaps it can be installed through conda ...\n",
    "\n",
    "FAQ 2: If you get the error `ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject` try to downgrade numpy for version `1.26.4`.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added datasets\\tutorial\\mipped\\S0_R0_C0.tif. Stage: 0, Round: 0, Channel: 0\n",
      "Added datasets\\tutorial\\mipped\\S0_R0_C1.tif. Stage: 0, Round: 0, Channel: 1\n",
      "Added datasets\\tutorial\\mipped\\S0_R0_C2.tif. Stage: 0, Round: 0, Channel: 2\n",
      "Added datasets\\tutorial\\mipped\\S0_R0_C3.tif. Stage: 0, Round: 0, Channel: 3\n",
      "Added datasets\\tutorial\\mipped\\S0_R0_C4.tif. Stage: 0, Round: 0, Channel: 4\n",
      "Added datasets\\tutorial\\mipped\\S1_R0_C0.tif. Stage: 1, Round: 0, Channel: 0\n",
      "Added datasets\\tutorial\\mipped\\S1_R0_C1.tif. Stage: 1, Round: 0, Channel: 1\n",
      "Added datasets\\tutorial\\mipped\\S1_R0_C2.tif. Stage: 1, Round: 0, Channel: 2\n",
      "Added datasets\\tutorial\\mipped\\S1_R0_C3.tif. Stage: 1, Round: 0, Channel: 3\n",
      "Added datasets\\tutorial\\mipped\\S1_R0_C4.tif. Stage: 1, Round: 0, Channel: 4\n",
      "Added datasets\\tutorial\\mipped\\S0_R1_C0.tif. Stage: 0, Round: 1, Channel: 0\n",
      "Added datasets\\tutorial\\mipped\\S0_R1_C1.tif. Stage: 0, Round: 1, Channel: 1\n",
      "Added datasets\\tutorial\\mipped\\S0_R1_C2.tif. Stage: 0, Round: 1, Channel: 2\n",
      "Added datasets\\tutorial\\mipped\\S0_R1_C3.tif. Stage: 0, Round: 1, Channel: 3\n",
      "Added datasets\\tutorial\\mipped\\S0_R1_C4.tif. Stage: 0, Round: 1, Channel: 4\n",
      "Added datasets\\tutorial\\mipped\\S1_R1_C0.tif. Stage: 1, Round: 1, Channel: 0\n",
      "Added datasets\\tutorial\\mipped\\S1_R1_C1.tif. Stage: 1, Round: 1, Channel: 1\n",
      "Added datasets\\tutorial\\mipped\\S1_R1_C2.tif. Stage: 1, Round: 1, Channel: 2\n",
      "Added datasets\\tutorial\\mipped\\S1_R1_C3.tif. Stage: 1, Round: 1, Channel: 3\n",
      "Added datasets\\tutorial\\mipped\\S1_R1_C4.tif. Stage: 1, Round: 1, Channel: 4\n",
      "Added datasets\\tutorial\\mipped\\S0_R2_C0.tif. Stage: 0, Round: 2, Channel: 0\n",
      "Added datasets\\tutorial\\mipped\\S0_R2_C1.tif. Stage: 0, Round: 2, Channel: 1\n",
      "Added datasets\\tutorial\\mipped\\S0_R2_C2.tif. Stage: 0, Round: 2, Channel: 2\n",
      "Added datasets\\tutorial\\mipped\\S0_R2_C3.tif. Stage: 0, Round: 2, Channel: 3\n",
      "Added datasets\\tutorial\\mipped\\S0_R2_C4.tif. Stage: 0, Round: 2, Channel: 4\n",
      "Added datasets\\tutorial\\mipped\\S1_R2_C0.tif. Stage: 1, Round: 2, Channel: 0\n",
      "Added datasets\\tutorial\\mipped\\S1_R2_C1.tif. Stage: 1, Round: 2, Channel: 1\n",
      "Added datasets\\tutorial\\mipped\\S1_R2_C2.tif. Stage: 1, Round: 2, Channel: 2\n",
      "Added datasets\\tutorial\\mipped\\S1_R2_C3.tif. Stage: 1, Round: 2, Channel: 3\n",
      "Added datasets\\tutorial\\mipped\\S1_R2_C4.tif. Stage: 1, Round: 2, Channel: 4\n",
      "Added datasets\\tutorial\\mipped\\S0_R3_C0.tif. Stage: 0, Round: 3, Channel: 0\n",
      "Added datasets\\tutorial\\mipped\\S0_R3_C1.tif. Stage: 0, Round: 3, Channel: 1\n",
      "Added datasets\\tutorial\\mipped\\S0_R3_C2.tif. Stage: 0, Round: 3, Channel: 2\n",
      "Added datasets\\tutorial\\mipped\\S0_R3_C3.tif. Stage: 0, Round: 3, Channel: 3\n",
      "Added datasets\\tutorial\\mipped\\S0_R3_C4.tif. Stage: 0, Round: 3, Channel: 4\n",
      "Added datasets\\tutorial\\mipped\\S1_R3_C0.tif. Stage: 1, Round: 3, Channel: 0\n",
      "Added datasets\\tutorial\\mipped\\S1_R3_C1.tif. Stage: 1, Round: 3, Channel: 1\n",
      "Added datasets\\tutorial\\mipped\\S1_R3_C2.tif. Stage: 1, Round: 3, Channel: 2\n",
      "Added datasets\\tutorial\\mipped\\S1_R3_C3.tif. Stage: 1, Round: 3, Channel: 3\n",
      "Added datasets\\tutorial\\mipped\\S1_R3_C4.tif. Stage: 1, Round: 3, Channel: 4\n",
      "Added datasets\\tutorial\\mipped\\S0_R4_C0.tif. Stage: 0, Round: 4, Channel: 0\n",
      "Added datasets\\tutorial\\mipped\\S0_R4_C1.tif. Stage: 0, Round: 4, Channel: 1\n",
      "Added datasets\\tutorial\\mipped\\S0_R4_C2.tif. Stage: 0, Round: 4, Channel: 2\n",
      "Added datasets\\tutorial\\mipped\\S0_R4_C3.tif. Stage: 0, Round: 4, Channel: 3\n",
      "Added datasets\\tutorial\\mipped\\S0_R4_C4.tif. Stage: 0, Round: 4, Channel: 4\n",
      "Added datasets\\tutorial\\mipped\\S1_R4_C0.tif. Stage: 1, Round: 4, Channel: 0\n",
      "Added datasets\\tutorial\\mipped\\S1_R4_C1.tif. Stage: 1, Round: 4, Channel: 1\n",
      "Added datasets\\tutorial\\mipped\\S1_R4_C2.tif. Stage: 1, Round: 4, Channel: 2\n",
      "Added datasets\\tutorial\\mipped\\S1_R4_C3.tif. Stage: 1, Round: 4, Channel: 3\n",
      "Added datasets\\tutorial\\mipped\\S1_R4_C4.tif. Stage: 1, Round: 4, Channel: 4\n",
      "    assembling thumbnail 2/2\n",
      "    aligning edge 1/1\n",
      "    assembling thumbnail 2/2\n",
      "    estimated cycle offset [y x] = [100.761604 -31.948801]\n",
      "    aligning tile 1/2"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\axela\\anaconda3\\envs\\iss_decoding_tutorial\\lib\\site-packages\\ashlar\\reg.py:1367: DataWarning: Could not align enough edges, proceeding anyway with original stage positions.\n",
      "  warnings.warn(message, DataWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    aligning tile 2/2\n",
      "    assembling thumbnail 2/2\n",
      "    estimated cycle offset [y x] = [ 93.3888   -29.491201]\n",
      "    aligning tile 2/2\n",
      "    assembling thumbnail 2/2\n",
      "    estimated cycle offset [y x] = [ 92.16   -28.2624]\n",
      "    aligning tile 2/2\n",
      "    assembling thumbnail 2/2\n",
      "    estimated cycle offset [y x] = [ 98.304  -22.1184]\n",
      "    aligning tile 2/2\n",
      "Cycle 0:\n",
      "    Channel 0:\n",
      "    Channel 1:\n",
      "    Channel 2:\n",
      "    Channel 3:\n",
      "    Channel 4:\n",
      "Cycle 1:\n",
      "    Channel 0:\n",
      "    Channel 1:\n",
      "    Channel 2:\n",
      "    Channel 3:\n",
      "    Channel 4:\n",
      "Cycle 2:\n",
      "    Channel 0:\n",
      "    Channel 1:\n",
      "    Channel 2:\n",
      "    Channel 3:\n",
      "    Channel 4:\n",
      "Cycle 3:\n",
      "    Channel 0:\n",
      "    Channel 1:\n",
      "    Channel 2:\n",
      "    Channel 3:\n",
      "    Channel 4:\n",
      "Cycle 4:\n",
      "    Channel 0:\n",
      "    Channel 1:\n",
      "    Channel 2:\n",
      "    Channel 3:\n",
      "    Channel 4:\n"
     ]
    }
   ],
   "source": [
    "from tools.image_container import ISSDataContainer\n",
    "from tools.stitching import stitch\n",
    "from os.path import join\n",
    "# First we load the miped data\n",
    "iss_data_miped = ISSDataContainer()\n",
    "iss_data_miped.add_images_from_filepattern(join('datasets','tutorial','mipped','S{stage}_R{round}_C{channel}.tif'))\n",
    "\n",
    "stage_locations = {\n",
    "    0: (0, 0), \n",
    "    1: (0, 1), \n",
    "}\n",
    "\n",
    "# Stitch using ASHLAR\n",
    "stitch(iss_data_miped, join('datasets','tutorial','stitched','R{round}_C{channel}.tif'), stage_locations, reference_channel=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Decoding\n",
    "\n",
    "In this step, we will proceed to decode the previously stitched image data. We start by creating a codebook, which can be thought of as a set of expected signal patterns across the rounds and channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from os.path import join\n",
    "from tools.decoding import Codebook\n",
    "\n",
    "# Load combinatorial labels (the codebook)\n",
    "# The metadata file is available in the Google Drive\n",
    "metadata = pickle.load(open(join('datasets','tutorial', 'decoding_tutorial', 'metadata.pkl'), 'rb'))\n",
    "\n",
    "# We need to create a 3D numpy array of shape (num_genes, num_rounds, num_channels)\n",
    "# that containes the combinatorial labells in a one-hot format\n",
    "num_rounds, num_channels = 5, 5\n",
    "codebook = Codebook(num_rounds, num_channels)\n",
    "for gene_id, (gene, indices) in enumerate(metadata['codebook'].items()):\n",
    "    r, c = indices['round_index'], indices['channel_index']\n",
    "    codeword = np.zeros((num_rounds, num_channels))\n",
    "    codeword[r,c] = 1.0\n",
    "    codebook.add_code(gene, codeword, is_unexpected='Negative' in gene)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added datasets\\tutorial\\stitched\\R0_C0.tif. Stage: 0, Round: 0, Channel: 0\n",
      "Added datasets\\tutorial\\stitched\\R0_C1.tif. Stage: 0, Round: 0, Channel: 1\n",
      "Added datasets\\tutorial\\stitched\\R0_C2.tif. Stage: 0, Round: 0, Channel: 2\n",
      "Added datasets\\tutorial\\stitched\\R0_C3.tif. Stage: 0, Round: 0, Channel: 3\n",
      "Added datasets\\tutorial\\stitched\\R0_C4.tif. Stage: 0, Round: 0, Channel: 4\n",
      "Added datasets\\tutorial\\stitched\\R1_C0.tif. Stage: 0, Round: 1, Channel: 0\n",
      "Added datasets\\tutorial\\stitched\\R1_C1.tif. Stage: 0, Round: 1, Channel: 1\n",
      "Added datasets\\tutorial\\stitched\\R1_C2.tif. Stage: 0, Round: 1, Channel: 2\n",
      "Added datasets\\tutorial\\stitched\\R1_C3.tif. Stage: 0, Round: 1, Channel: 3\n",
      "Added datasets\\tutorial\\stitched\\R1_C4.tif. Stage: 0, Round: 1, Channel: 4\n",
      "Added datasets\\tutorial\\stitched\\R2_C0.tif. Stage: 0, Round: 2, Channel: 0\n",
      "Added datasets\\tutorial\\stitched\\R2_C1.tif. Stage: 0, Round: 2, Channel: 1\n",
      "Added datasets\\tutorial\\stitched\\R2_C2.tif. Stage: 0, Round: 2, Channel: 2\n",
      "Added datasets\\tutorial\\stitched\\R2_C3.tif. Stage: 0, Round: 2, Channel: 3\n",
      "Added datasets\\tutorial\\stitched\\R2_C4.tif. Stage: 0, Round: 2, Channel: 4\n",
      "Added datasets\\tutorial\\stitched\\R3_C0.tif. Stage: 0, Round: 3, Channel: 0\n",
      "Added datasets\\tutorial\\stitched\\R3_C1.tif. Stage: 0, Round: 3, Channel: 1\n",
      "Added datasets\\tutorial\\stitched\\R3_C2.tif. Stage: 0, Round: 3, Channel: 2\n",
      "Added datasets\\tutorial\\stitched\\R3_C3.tif. Stage: 0, Round: 3, Channel: 3\n",
      "Added datasets\\tutorial\\stitched\\R3_C4.tif. Stage: 0, Round: 3, Channel: 4\n",
      "Added datasets\\tutorial\\stitched\\R4_C0.tif. Stage: 0, Round: 4, Channel: 0\n",
      "Added datasets\\tutorial\\stitched\\R4_C1.tif. Stage: 0, Round: 4, Channel: 1\n",
      "Added datasets\\tutorial\\stitched\\R4_C2.tif. Stage: 0, Round: 4, Channel: 2\n",
      "Added datasets\\tutorial\\stitched\\R4_C3.tif. Stage: 0, Round: 4, Channel: 3\n",
      "Added datasets\\tutorial\\stitched\\R4_C4.tif. Stage: 0, Round: 4, Channel: 4\n",
      "Decoding tile: 1\n"
     ]
    }
   ],
   "source": [
    "from tools.decoding import istdeco, calculate_fdr\n",
    "from tools.image_container import ISSDataContainer\n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "# Load the stitched data\n",
    "issdata = ISSDataContainer().add_images_from_filepattern(join('datasets','tutorial', 'stitched','R{round}_C{channel}.tif'))\n",
    "\n",
    "# Run the decoding\n",
    "results = []\n",
    "tile_idx = 1\n",
    "\n",
    "\n",
    "for tile, origin in issdata.iterate_tiles(tile_height=512, tile_width=512, squeeze=True, use_vips=True):\n",
    "    print(f'Decoding tile: {tile_idx}')\n",
    "    tile_idx += 1\n",
    "    # Decode the data using matrix factorization\n",
    "    # Depending on your data, you might want to adjust the parameter min_integrated_intensity\n",
    "    # or min_score\n",
    "    # Usually a score threshold between 0.5 and 0.85 works fine. \n",
    "    # This is really slow unless we can run on the GPU.\n",
    "    decoded_table = istdeco(tile, codebook, spot_sigma=2, device='cuda')\n",
    "\n",
    "    decoded_table['Y'] = decoded_table['Y'] + origin[0]\n",
    "    decoded_table['X'] = decoded_table['X'] + origin[1]\n",
    "    results.append(pd.DataFrame(decoded_table))\n",
    "    break\n",
    "    # Remove this to run over everything\n",
    "\n",
    "results = pd.concat(results, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the genes are marked as `Unexpected` in the codebook. These genes correspond to non-biological labels that we do not expect to find in the data. Treating these unexpected genes as false-positives allow us to estimate a false discovery rate. This value is useful for quality control. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False discovery rate is: 0.4565630944803008\n"
     ]
    }
   ],
   "source": [
    "from tools.decoding import calculate_fdr, filter_to_fdr\n",
    "fdr = calculate_fdr(results['Name'], codebook.get_unexpected())\n",
    "print(f'False discovery rate is: {fdr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also compute the quality for a different quality threshold \n",
    "filtered_results, optimal_quality, optimal_intensity_threshold = filter_to_fdr(results, codebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_results.to_csv(join('datasets','tutorial','stitched','results.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioinformatics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
